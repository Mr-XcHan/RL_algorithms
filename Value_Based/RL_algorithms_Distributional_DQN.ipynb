{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    algorithm: Distributional DQN(C51)\n",
    "        In the algorithm，the Q network accept a state, and its outputs are not Q(s,a) but a distribution of this state.\n",
    "This distribution describes the possibility of all values of the state for every action.\n",
    "More details you can learn from the paper: https://arxiv.org/pdf/1707.06887.pdf\n",
    "        key points:\n",
    "            1. Q-value --> Q-distribution\n",
    "\n",
    "    environment: CartPole-v0\n",
    "    state:\n",
    "        1.Cart Position:[-4.8,4.8],  2.Cart Velocity[-Inf,Inf],  3.Pole Angle[-24 deg, 24 deg]\n",
    "        4.Pole Velocity [-Inf,Inf]\n",
    "    action:\n",
    "        0: left    \n",
    "        1: right\n",
    "    reward: 1 for every step    \n",
    "        \n",
    "    prerequisites: tensorflow 2.2(tensorflow >= 2.0)\n",
    "    notice：\n",
    "    \n",
    "    author: Xinchen Han\n",
    "    date: 2020/7/30\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Environment\"\"\"\n",
    "env = gym.make('CartPole-v0')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "\"\"\"Random seed\"\"\"\n",
    "env.seed(6)\n",
    "np.random.seed(6)\n",
    "random.seed(6)\n",
    "tf.random.set_seed(6)\n",
    "\n",
    "\"\"\"Set hyperparameters\"\"\"\n",
    "alpha = 0.9\n",
    "gamma = 0.9\n",
    "max_episodes = 500\n",
    "\n",
    "render = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distributional_DQN(object):\n",
    "    def __init__(self):\n",
    "        self.step = 0\n",
    "        self.batch_size = 64\n",
    "        self.update_freq = 100\n",
    "        self.replay_size = 1000\n",
    "        self.atom_num = 51\n",
    "        self.replay_queue = deque(maxlen=self.replay_size)\n",
    "        self.model = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "        self.optimizer = keras.optimizers.Adam(1e-3)\n",
    "        self.min_value = -10\n",
    "        self.max_value = 10\n",
    "        self.vrange = np.linspace(self.min_value, self.max_value, self.atom_num)\n",
    "        self.deltaz = float(self.max_value - self.min_value) / (self.atom_num - 1)\n",
    "\n",
    "    def create_model(self):\n",
    "        input = keras.layers.Input(shape=state_dim)\n",
    "        hidden1 = keras.layers.Dense(64, activation='relu')(input)\n",
    "        hidden2 = keras.layers.Dense(16, activation='tanh')(hidden1)\n",
    "        hidden3 = keras.layers.Dense(action_dim * self.atom_num)(hidden2)\n",
    "        reshape = keras.layers.Reshape((action_dim, self.atom_num))(hidden3)\n",
    "        output = tf.nn.log_softmax(reshape, 2)\n",
    "        model = keras.models.Model(inputs=[input], outputs=[output])\n",
    "        return model\n",
    "\n",
    "    def choose_action(self, state, epsilon=0.1):\n",
    "        if np.random.uniform() > epsilon - self.step * 0.0001:\n",
    "            qdist = np.exp(self.model(state.reshape(1,4)).numpy())\n",
    "            qvalues = (qdist * self.vrange).sum(-1)\n",
    "            return qvalues.argmax(1)[0]\n",
    "        else:\n",
    "            return random.sample(list(np.arange(0, action_dim)), 1)[0]\n",
    "\n",
    "    def fill_replay(self, state, action, state_, reward, done):\n",
    "        self.replay_queue.append((state, action, state_, reward, done))\n",
    "\n",
    "    def model_train(self):\n",
    "        self.step += 1\n",
    "        if self.step % self.update_freq == 0:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        replay_batch = random.sample(self.replay_queue, self.batch_size)\n",
    "        state_batch = np.array([replay[0] for replay in replay_batch])\n",
    "        action_batch = np.array([replay[1] for replay in replay_batch])\n",
    "        next_state_batch = np.array([replay[2] for replay in replay_batch])\n",
    "        reward_batch = np.array([replay[3] for replay in replay_batch])\n",
    "        done_batch = np.array([replay[4] for replay in replay_batch])\n",
    "\n",
    "        b_dist_ = np.exp(self.target_model(next_state_batch).numpy())\n",
    "        b_a_ = (b_dist_ * self.vrange).sum(-1).argmax(1)\n",
    "        b_tzj = np.clip(gamma * (1 - done_batch[:, None]) * self.vrange[None, :] + reward_batch[:, None],\n",
    "                        self.min_value, self.max_value)\n",
    "        b_i = (b_tzj - self.min_value) / self.deltaz\n",
    "        b_l = np.floor(b_i).astype('int64')\n",
    "        b_u = np.ceil(b_i).astype('int64')\n",
    "        templ = b_dist_[range(self.batch_size), b_a_, :] * (b_u - b_i)\n",
    "        tempu = b_dist_[range(self.batch_size), b_a_, :] * (b_i - b_l)\n",
    "        b_m = np.zeros((self.batch_size, self.atom_num))\n",
    "\n",
    "        for j in range(self.batch_size):\n",
    "            for k in range(self.atom_num):\n",
    "                b_m[j][b_l[j][k]] += templ[j][k]\n",
    "                b_m[j][b_u[j][k]] += tempu[j][k]\n",
    "        b_m = tf.convert_to_tensor(b_m, dtype='float32')\n",
    "        b_index = np.stack([range(self.batch_size), action_batch], 1)\n",
    "        b_index = tf.convert_to_tensor(b_index, 'int64')\n",
    "\n",
    "        self._train_func(state_batch, b_index, b_m)\n",
    "\n",
    "    @tf.function\n",
    "    def _train_func(self, b_o, b_index, b_m):\n",
    "        with tf.GradientTape() as tape:\n",
    "            b_dist_a = tf.gather_nd(self.model(b_o), b_index)\n",
    "            loss = tf.reduce_mean(tf.negative(tf.reduce_sum(b_dist_a * b_m, 1)))\n",
    "\n",
    "        grad = tape.gradient(loss, self.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grad, self.model.trainable_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    agent = Distributional_DQN()\n",
    "    score_list = []\n",
    "    for episode in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        while True:\n",
    "            action = agent.choose_action(state)\n",
    "            if render:\n",
    "                env.render()\n",
    "            state_, reward, done, _ = env.step(action)\n",
    "            agent.fill_replay(state, action, state_, reward, done)\n",
    "            if len(agent.replay_queue) >= agent.replay_size:\n",
    "                agent.model_train()\n",
    "            state = state_\n",
    "            score += reward\n",
    "            if done:\n",
    "                score_list.append(score)\n",
    "                print('episode:', episode, 'score:', score, 'max_score:', np.max(score_list))\n",
    "                if len(agent.replay_queue) >= agent.replay_size:\n",
    "                    print(\"   Training   ....\")\n",
    "                break\n",
    "        if np.mean(score_list[-10:]) > 180:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    plt.plot(score_list, color='orange')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
