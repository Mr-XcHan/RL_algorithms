{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "    algorithm：SARSA\n",
    "        Q(s,a) <- Q(s,a) + alpha(r + gamma * Q(s_, a_) - Q(s, a))\n",
    "    \n",
    "    environment：FrozenLake-v0\n",
    "\n",
    "    author: Xinchen Han\n",
    "    date: 2020/7/25\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg_name = 'SARSA'\n",
    "env_id = 'FrozenLake-v0'\n",
    "env = gym.make(env_id)\n",
    "\n",
    "## Set hyperparameters\n",
    "epsilon = .8\n",
    "alpha = .8  \n",
    "gamma = .9  # decay factor\n",
    "max_episodes = 20000\n",
    "t0 = time.time()\n",
    "\n",
    "Q_table = np.zeros([env.observation_space.n, env.action_space.n], dtype = np.float64)\n",
    "reward_buffer = [0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "\n",
    "    if (np.random.rand() > epsilon) or ((Q_table[state, :] == 0)).all():\n",
    "        action = np.random.choice(env.action_space.n)\n",
    "    else:\n",
    "        action = np.argmax(Q_table[state,:])\n",
    "    return action\n",
    "\n",
    "def SARSA():\n",
    "    for episode in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        action = np.argmax(Q_table[state, :] + np.random.randn(1, env.action_space.n) * (1. / (episode + 1)))\n",
    "        epi_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "#             action = choose_action(state)  \n",
    "#             action = np.argmax(Q_table[state, :] + np.random.randn(1, env.action_space.n) * (1. / (episode + 1)))\n",
    "            state_, reward, done, _ = env.step(action)\n",
    "            #   greedy policy add a noisy\n",
    "            action_ = np.argmax(Q_table[state_, :] + np.random.randn(1, env.action_space.n) * (1. / (episode + 1))) \n",
    "            Q_table[state][action] = Q_table[state][action] + \\\n",
    "                                        alpha * (reward + gamma * Q_table[state_, action_] - Q_table[state][action])\n",
    "            state = state_\n",
    "            action = action_\n",
    "            epi_reward += reward\n",
    "        reward_buffer.append(reward_buffer[-1] * 0.9 + epi_reward * 0.1)\n",
    "        print(\n",
    "                'Training  | Episode: {}/{}  | Reward:{: .4f} |Running Time: {:.4f}'.format(\n",
    "                episode + 1, max_episodes, epi_reward, time.time() - t0))\n",
    "    print(\"Trained mean reward :{:.4f}\".format(np.mean(reward_buffer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SARSA()\n",
    "plt.plot(reward_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
