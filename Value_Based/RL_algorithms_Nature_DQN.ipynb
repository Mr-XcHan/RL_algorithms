{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    algorithm: Nature_DQN( two Q networks) \n",
    "        Nature_DQN have two network, in this case we can avoid the circular dependency between the target Q and \n",
    "evaluation Q. If only a network, the TD-error will be greater deviation. but the Nature DQN is different from the DDQN.\n",
    "You can see more detail at:  https://www.cnblogs.com/pinard/p/9756075.html.\n",
    "    three key points:\n",
    "        1. function approximation\n",
    "        2. target Q network\n",
    "        3. experience replay\n",
    "\n",
    "    environment: MountainCar-v0\n",
    "        State: [position, velocity]， position[-0.6, 0.6]， velocity[-0.1, 0.1]\n",
    "        Action: 0(left)  or  1(stop)  or  2(right)\n",
    "        Reward: -1 for every step\n",
    "        Done: get hilltop\n",
    "        \n",
    "    prerequisites:  tensorflow 2.2(tensorflow >= 2.0)\n",
    "    notice： don‘t env.make(MountainCar).unwrapper, which will cost a huge amount of time.\n",
    "    \n",
    "    author: Xinchen Han\n",
    "\n",
    "    date: 2020/7/27\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import gym\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 3\n",
    "\n",
    "env_id = 'MountainCar-v0'\n",
    "env = gym.make(env_id)\n",
    "# env = env.unwrapped\n",
    "# print(\"the dimension of the state\",env.observation_space.shape[0])\n",
    "# print(\"the dimension of the action\",env.action_space)\n",
    "max_episodes = 500\n",
    "\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "render = True\n",
    "state_dim = 2\n",
    "action_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nature_DQN(object):\n",
    "    def __init__(self):\n",
    "        self.step = 0\n",
    "        self.update_freq = 100\n",
    "        self.replay_size = 2000\n",
    "        self.replay_queue = deque(maxlen=self.replay_size)\n",
    "        self.model = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(1e-2)\n",
    "\n",
    "    def create_model(self):\n",
    "        input = keras.layers.Input(shape=state_dim)\n",
    "        hidden1 = keras.layers.Dense(64, activation='tanh')(input)\n",
    "        hidden2 = keras.layers.Dense(16, activation='relu')(hidden1)\n",
    "        output = keras.layers.Dense(action_dim)(hidden2)\n",
    "        model = keras.models.Model(inputs=[input], outputs=[output])\n",
    "        return model\n",
    "\n",
    "    def choose_action(self, state, epsilon=0.1):\n",
    "        if np.random.uniform() < epsilon - self.step * 0.0001:\n",
    "            return np.random.choice([0, 1, 2])\n",
    "        return np.argmax(self.model.predict(np.array([state]))[0])\n",
    "\n",
    "    def fill_queue(self, state, action, state_, reward, done):\n",
    "        if state_[0] >= 0.4:\n",
    "            reward += 1\n",
    "        self.replay_queue.append((state, action, state_, reward, done))\n",
    "\n",
    "    def train(self, batch_size=64, gamma=0.95):\n",
    "        if len(self.replay_queue) < self.replay_size:\n",
    "            return\n",
    "        self.step += 1\n",
    "        if self.step % self.update_freq == 0:  #  have trained self.update_freq times to update target_model\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        replay_batch = random.sample(self.replay_queue, batch_size)\n",
    "        state_batch = np.array([replay[0] for replay in replay_batch])\n",
    "        action_batch = np.array([replay[1] for replay in replay_batch])\n",
    "        next_state_batch = np.array([replay[2] for replay in replay_batch])\n",
    "        reward_batch = np.array([replay[3] for replay in replay_batch])\n",
    "        done_batch = np.array([replay[4] for replay in replay_batch])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.model.variables)\n",
    "            Q = self.model(tf.convert_to_tensor(state_batch, dtype=tf.float32))\n",
    "            Q_next = self.target_model(tf.convert_to_tensor(next_state_batch, dtype=tf.float32))\n",
    "\n",
    "            max_action = tf.argmax(Q_next, axis=1)\n",
    "            Q = tf.reduce_sum(tf.one_hot(action_batch, action_dim) * Q, axis=1)\n",
    "            Q_next = tf.reduce_sum(tf.one_hot(max_action, action_dim) * Q_next, axis=1)\n",
    "\n",
    "            target_value = (1 - done_batch) * gamma * Q_next + reward_batch\n",
    "            loss = tf.reduce_mean(tf.square(Q - target_value) * 0.5)\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.variables))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    agent = Nature_DQN()\n",
    "    score_buffer = []\n",
    "    for episode in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        while True:\n",
    "            action = agent.choose_action(state)\n",
    "            if render:\n",
    "                env.render()\n",
    "            state_, reward, done, _ = env.step(action)\n",
    "            agent.fill_queue(state, action, state_, reward, done)\n",
    "            agent.train()\n",
    "            score += reward\n",
    "            state = state_\n",
    "            if done:\n",
    "                score_buffer.append(score)\n",
    "                print('episode:', episode, 'score:', score, 'max:', np.max(score_buffer))\n",
    "                break\n",
    "\n",
    "        if np.mean(score_buffer[-20:]) > -160:\n",
    "            break\n",
    "    env.close()\n",
    "    plt.plot(score_buffer)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
