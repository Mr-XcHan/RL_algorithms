{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Rainbow:   \\n        1. Double\\n        2. Dueling\\n        3. PrioritizedExperienceReplay\\n        4. N-Step\\n        5. Distributional\\n        6. Noisy Net\\n        \\n     More details you can learn from the paper : https://arxiv.org/abs/1710.02298\\n     \\n    This code is from:\\n    https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn_prioritised_replay.py\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Rainbow:   \n",
    "        1. Double\n",
    "        2. Dueling\n",
    "        3. PrioritizedExperienceReplay\n",
    "        4. N-Step\n",
    "        5. Distributional\n",
    "        6. Noisy Net\n",
    "        \n",
    "     More details you can learn from the paper : https://arxiv.org/abs/1710.02298\n",
    "     \n",
    "    This code is from:\n",
    "       https://github.com/chagmgang/tf2.0_reinforcement_learning/blob/master/rainbow/rainbow.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import optimizers, losses\n",
    "from tensorflow.keras import Model\n",
    "from collections import deque\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import gym\n",
    "\n",
    "\n",
    "class SumTree:\n",
    "    write = 0\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.n_entries = 0\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "        self.tree[parent] += change\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])\n",
    "\n",
    "\n",
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    e = 0.001\n",
    "    a = 0.6\n",
    "    beta = 0.4\n",
    "    beta_increment_per_sampling = 0.001\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def reset(self):\n",
    "        self.tree = SumTree(self.capacity)\n",
    "\n",
    "    def _getPriority(self, error):\n",
    "        return (error + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.tree.total() / n\n",
    "        priorities = []\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            priorities.append(p)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "\n",
    "        sampling_probabilities = priorities / self.tree.total()\n",
    "        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)\n",
    "        is_weight /= is_weight.max()\n",
    "\n",
    "        return batch, idxs, is_weight\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.update(idx, p)\n",
    "\n",
    "class n_step_memory:\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.state = collections.deque(maxlen=int(maxlen))\n",
    "        self.action = collections.deque(maxlen=int(maxlen))\n",
    "        self.reward = collections.deque(maxlen=int(maxlen))\n",
    "        self.next_state = collections.deque(maxlen=int(maxlen))\n",
    "        self.done = collections.deque(maxlen=int(maxlen))\n",
    "\n",
    "    def append(self, state, next_state, reward, done, action):\n",
    "        self.state.append(state)\n",
    "        self.action.append(action)\n",
    "        self.reward.append(reward)\n",
    "        self.next_state.append(next_state)\n",
    "        self.done.append(done)\n",
    "\n",
    "    def sample(self):\n",
    "        if len(self.state) == int(self.maxlen):\n",
    "            done = [self.done[i] for i in range(self.maxlen-1)]\n",
    "            if True in done:\n",
    "                return None\n",
    "            return {'state': np.stack(self.state),\n",
    "                    'next_state': np.stack(self.next_state),\n",
    "                    'reward': np.stack(self.reward),\n",
    "                    'done': np.stack(self.done),\n",
    "                    'action': np.stack(self.action)}\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "class Embedding(Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.layer = tf.keras.layers.Dense(embedding_dim, activation='relu')\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def call(self, batch_size, num_quantile, tau_min, tau_max):\n",
    "        sample = tf.random.uniform(\n",
    "            [batch_size * num_quantile, 1],\n",
    "            minval=tau_min, maxval=tau_max, dtype=tf.float32)\n",
    "        sample_tile = tf.tile(sample, [1, self.embedding_dim])\n",
    "        embedding = tf.cos(\n",
    "            tf.cast(tf.range(0, self.embedding_dim, 1), tf.float32) * np.pi * sample_tile)\n",
    "        embedding_out = self.layer(embedding)\n",
    "        return embedding_out, sample\n",
    "\n",
    "class IQN(Model):\n",
    "    def __init__(self):\n",
    "        super(IQN, self).__init__()\n",
    "        \n",
    "        self.num_action = 2\n",
    "        self.embedding_dim = 64\n",
    "\n",
    "        self.embedding_out = Embedding(self.embedding_dim)\n",
    "\n",
    "        self.layer1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.layer2 = tf.keras.layers.Dense(64, activation='relu')\n",
    "\n",
    "        self.h_fc1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.state = tf.keras.layers.Dense(self.num_action)\n",
    "        self.advantage = tf.keras.layers.Dense(self.num_action)\n",
    "\n",
    "\n",
    "    def call(self, state, num_quantile, tau_min, tau_max):\n",
    "        layer1 = self.layer1(state)\n",
    "        h_flat = self.layer2(layer1)\n",
    "        h_flat_tile = tf.tile(h_flat, [num_quantile, 1])\n",
    "        \n",
    "        embedding_out, sample = self.embedding_out(\n",
    "            state.shape[0], num_quantile, tau_min, tau_max)\n",
    "        \n",
    "        h_flat_embedding = tf.multiply(h_flat_tile, embedding_out)\n",
    "        \n",
    "        h_fc1 = self.h_fc1(h_flat_embedding)\n",
    "        logits_state = self.state(h_fc1)\n",
    "        logits_hidden = self.advantage(h_fc1)\n",
    "        mean = tf.expand_dims(tf.reduce_mean(logits_hidden, axis=1), axis=1)\n",
    "        advantage = (logits_hidden - mean)\n",
    "\n",
    "        logits = logits_state + advantage\n",
    "\n",
    "        logits_reshape = tf.reshape(logits, [num_quantile, state.shape[0], self.num_action])\n",
    "        \n",
    "        Q_action = tf.reduce_mean(logits_reshape, axis=0)\n",
    "\n",
    "        return logits_reshape, Q_action, sample\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.lr = 0.00025\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        self.get_action_num_quantile = 32\n",
    "        self.get_action_tau_min = 0.0\n",
    "        self.get_action_tau_max = 0.25\n",
    "\n",
    "        self.train_num_quantile = 8\n",
    "        self.train_tau_min = 0.0\n",
    "        self.train_tau_max = 1.0\n",
    "\n",
    "        self.train_num_quantile = 8\n",
    "        self.batch_size = 64\n",
    "        self.state_size = 4\n",
    "        self.action_size = 2\n",
    "        self.n_step = 5\n",
    "\n",
    "        self.iqn_model = IQN()\n",
    "        self.iqn_target = IQN()\n",
    "        self.opt = optimizers.Adam(lr=self.lr, epsilon=(1e-2/self.batch_size))\n",
    "\n",
    "        self.n_step_memory = n_step_memory(maxlen=int(self.n_step))\n",
    "        self.memory = Memory(capacity=int(2000))\n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.n_step_memory.append(state, next_state, reward, done, action)\n",
    "        n_step_sample = self.n_step_memory.sample()\n",
    "        if not n_step_sample is None:\n",
    "            state = n_step_sample['state'][0]\n",
    "            next_state = n_step_sample['next_state'][-1]\n",
    "            reward = n_step_sample['reward']\n",
    "            done = n_step_sample['done'][-1]\n",
    "            action = n_step_sample['action'][0]\n",
    "\n",
    "            reward = np.sum([np.power(self.gamma, i) * r for i, r in enumerate(reward)])\n",
    "\n",
    "            _, Q_batch, _ = self.iqn_model(\n",
    "            tf.convert_to_tensor([next_state], dtype=tf.float32),\n",
    "            self.get_action_num_quantile, self.get_action_tau_min,\n",
    "            self.get_action_tau_max)\n",
    "\n",
    "            Q_batch = np.array(Q_batch)[0]\n",
    "            next_action = np.argmax(Q_batch)\n",
    "\n",
    "            _, target_Q_batch, _ = self.iqn_target(\n",
    "            tf.convert_to_tensor([next_state], dtype=tf.float32),\n",
    "            self.get_action_num_quantile, self.get_action_tau_min,\n",
    "            self.get_action_tau_max)\n",
    "\n",
    "            target_Q_batch = np.array(target_Q_batch)[0]\n",
    "            target_value = target_Q_batch[next_action]\n",
    "\n",
    "            target_value = target_value * np.power(self.gamma, self.n_step) * (1-done) + reward\n",
    "        \n",
    "            _, Q_batch, _ = self.iqn_model(\n",
    "                tf.convert_to_tensor([state], dtype=tf.float32),\n",
    "                self.get_action_num_quantile, self.get_action_tau_min,\n",
    "                self.get_action_tau_max)\n",
    "        \n",
    "            main_q = np.array(Q_batch)[0]\n",
    "            main_q = main_q[action]\n",
    "\n",
    "            td_error = np.abs(target_value - main_q)\n",
    "\n",
    "            self.memory.add(td_error, (state, action, reward, next_state, done))\n",
    "\n",
    "    def get_action(self, state, epsilon):\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        _, q_value, _ = self.iqn_model(\n",
    "                        state, self.get_action_num_quantile,\n",
    "                        self.get_action_tau_min, self.get_action_tau_max)\n",
    "        q_value = q_value[0]\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.choice(self.action_size)\n",
    "        else:\n",
    "            action = np.argmax(q_value)\n",
    "        \n",
    "        return action, q_value\n",
    "\n",
    "    def update_target(self):\n",
    "        self.iqn_target.set_weights(self.iqn_model.get_weights())\n",
    "\n",
    "    def update(self):\n",
    "        mini_batch, idxs, IS_weight = self.memory.sample(self.batch_size)\n",
    "\n",
    "        states = np.stack([i[0] for i in mini_batch])\n",
    "        actions = np.stack([i[1] for i in mini_batch])\n",
    "        rewards = np.stack([i[2] for i in mini_batch])\n",
    "        next_states = np.stack([i[3] for i in mini_batch])\n",
    "        dones = np.stack([i[4] for i in mini_batch])\n",
    "\n",
    "        _, Q_batch, _ = self.iqn_model(\n",
    "            tf.convert_to_tensor(np.stack(next_states), dtype=tf.float32),\n",
    "            self.train_num_quantile, self.train_tau_min,\n",
    "            self.train_tau_max)\n",
    "        theta_batch, next_target_Q, _ = self.iqn_target(\n",
    "            tf.convert_to_tensor(np.stack(next_states), dtype=tf.float32),\n",
    "            self.train_num_quantile, self.train_tau_min,\n",
    "            self.train_tau_max)\n",
    "\n",
    "        Q_batch, theta_batch = np.array(Q_batch), np.array(theta_batch)\n",
    "        next_target_Q = np.array(next_target_Q)\n",
    "        next_action = np.argmax(next_target_Q, axis=1)\n",
    "        next_value = np.stack([t[a] for a, t in zip(next_action, next_target_Q)])\n",
    "\n",
    "        target_value_list = []\n",
    "        for r, d, nv in zip(rewards, dones, next_value):\n",
    "            target_value_list.append(r + np.power(self.gamma, self.n_step) * (1-d) * nv)\n",
    "        target_value_list = np.stack(target_value_list)\n",
    "\n",
    "        theta_target = []\n",
    "        for i in range(len(mini_batch)):\n",
    "            theta_target.append([])\n",
    "            for j in range(self.train_num_quantile):\n",
    "                target_value = rewards[i] + np.power(self.gamma, self.n_step) * (1-dones[i]) * theta_batch[j, i, np.argmax(Q_batch[i])]\n",
    "                theta_target[i].append(target_value)\n",
    "\n",
    "        action_binary = np.zeros([self.train_num_quantile, len(mini_batch), self.action_size])\n",
    "        for i in range(len(actions)):\n",
    "            action_binary[:, i, actions[i]] = 1\n",
    "\n",
    "        iqn_variable = self.iqn_model.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            theta_target = tf.convert_to_tensor(theta_target, dtype=tf.float32)\n",
    "            action_binary_loss = tf.convert_to_tensor(action_binary, dtype=tf.float32)\n",
    "            logits, _, sample = self.iqn_model(\n",
    "                        tf.convert_to_tensor(np.stack(states), dtype=tf.float32),\n",
    "                        self.train_num_quantile, self.train_tau_min,\n",
    "                        self.train_tau_max)\n",
    "            theta_pred = tf.reduce_sum(tf.multiply(logits, action_binary_loss), axis=2)\n",
    "\n",
    "            theta_target_tile = tf.tile(tf.expand_dims(theta_target, axis=0), [self.train_num_quantile, 1, 1])\n",
    "            theta_pred_tile = tf.tile(tf.expand_dims(theta_pred, axis=2), [1, 1, self.train_num_quantile])\n",
    "\n",
    "            error_loss = theta_target_tile - theta_pred_tile\n",
    "\n",
    "            Huber_loss = tf.compat.v1.losses.huber_loss(theta_target_tile, theta_pred_tile, reduction = tf.losses.Reduction.NONE)\n",
    "\n",
    "            tau = tf.reshape(sample, [self.train_num_quantile, -1, 1])\n",
    "            tau = tf.tile(tau, [1, 1, self.train_num_quantile])\n",
    "            inv_tau = 1.0 - tau\n",
    "\n",
    "            Loss = tf.where(tf.less(error_loss, 0.0), inv_tau * Huber_loss, tau * Huber_loss)\n",
    "            unweighted_loss = tf.reduce_sum(tf.reduce_mean(Loss, axis=-1), axis=0)\n",
    "            weight = tf.convert_to_tensor(IS_weight, dtype=tf.float32)\n",
    "            Loss = tf.reduce_mean(unweighted_loss * weight)\n",
    "\n",
    "        grads = tape.gradient(Loss, iqn_variable)\n",
    "        self.opt.apply_gradients(zip(grads, iqn_variable))\n",
    "\n",
    "        _, main_q, _ = self.iqn_model(\n",
    "            tf.convert_to_tensor(states, dtype=tf.float32),\n",
    "            self.train_num_quantile, self.train_tau_min,\n",
    "            self.train_tau_max)\n",
    "\n",
    "        main_q = np.array(main_q)\n",
    "        main_q = np.stack([q[a] for a, q in zip(actions, Q_batch)])\n",
    "\n",
    "        td_error = np.abs(target_value_list - main_q)\n",
    "        \n",
    "        for i in range(len(mini_batch)):\n",
    "            idx = idxs[i]\n",
    "            self.memory.update(idx, td_error[i])\n",
    "        \n",
    "    def run(self):\n",
    "\n",
    "        env = gym.make('CartPole-v1')\n",
    "        episode = 0\n",
    "        step = 0\n",
    "\n",
    "        while True:\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode += 1\n",
    "            epsilon = 1 / (episode * 0.1 + 1)\n",
    "            score = 0\n",
    "            while not done:\n",
    "                step += 1\n",
    "                action, q_value = self.get_action(state, epsilon)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "\n",
    "                self.append_sample(state, action, reward, next_state, done)\n",
    "                \n",
    "                score += reward\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if step > 100:\n",
    "                    self.update()\n",
    "                    if step % 20 == 0:\n",
    "                        self.update_target()\n",
    "\n",
    "            print(episode, score)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    agent = Agent()\n",
    "    agent.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
