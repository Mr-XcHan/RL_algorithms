{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    algorithm: Prioritized Replay DDQN\n",
    "        The experience will be sampled according to the TD-error.\n",
    "        The greater the difference of TD-error(s,a), the higher the priority of (s,a) sampled.\n",
    "        Certainly， the Loss function also include the weight of the priority.\n",
    "\n",
    "        More details you can learn from the paper: https://arxiv.org/pdf/1511.05952\n",
    "        key points:\n",
    "            1. The method of sampled experience is imporved.\n",
    "        \n",
    "    environment: CartPole-v0\n",
    "    state:\n",
    "        1.Cart Position:[-4.8,4.8],  2.Cart Velocity[-Inf,Inf],  3.Pole Angle[-24 deg, 24 deg]\n",
    "        4.Pole Velocity [-Inf,Inf]\n",
    "    action:\n",
    "        0: left    \n",
    "        1: right\n",
    "    reward: 1 for every step    \n",
    "        \n",
    "    prerequisites:  tensorflow 2.2(tensorflow >= 2.0)\n",
    "    notice：\n",
    "    \n",
    "    author: Xinchen Han\n",
    "    date: 2020/7/30\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Environment\"\"\"\n",
    "env = gym.make('CartPole-v0')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "\"\"\"Random seed\"\"\"\n",
    "env.seed(6)\n",
    "np.random.seed(6)\n",
    "random.seed(6)\n",
    "tf.random.set_seed(6)\n",
    "\n",
    "\"\"\"Set hyperparameters\"\"\"\n",
    "alpha = 0.9\n",
    "gamma = 0.9\n",
    "max_episodes = 500\n",
    "\n",
    "render = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is from:\n",
    "    https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn_prioritised_replay.py\n",
    "    Story data with its priority in the tree.\n",
    "    \"\"\"\n",
    "\n",
    "    data_pointer = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity  # for all priority values\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        # [--------------Parent nodes-------------][-------leaves to recode priority-------]\n",
    "        #             size: capacity - 1                       size: capacity\n",
    "        self.data = np.zeros(capacity, dtype=object)  # for all agent info.\n",
    "        # [--------------data frame-------------]\n",
    "        #             size: capacity\n",
    "\n",
    "    def add(self, p, data):\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data  # update data_frame\n",
    "        self.update(tree_idx, p)  # update tree_frame\n",
    "\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:  # replace when exceed the capacity\n",
    "            self.data_pointer = 0\n",
    "\n",
    "    def update(self, tree_idx, p):\n",
    "        change = p - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = p\n",
    "        # then propagate the change through tree\n",
    "        while tree_idx != 0:    # this method is faster than the recursive loop in the reference code\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for transitions\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        parent_idx = 0\n",
    "        while True:     # the while loop is faster than the method in the reference code\n",
    "            cl_idx = 2 * parent_idx + 1         # this leaf's left and right kids\n",
    "            cr_idx = cl_idx + 1\n",
    "            if cl_idx >= len(self.tree):        # reach bottom, end search\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:       # downward search, always search for a higher priority node\n",
    "                if v <= self.tree[cl_idx]:\n",
    "                    parent_idx = cl_idx\n",
    "                else:\n",
    "                    v -= self.tree[cl_idx]\n",
    "                    parent_idx = cr_idx\n",
    "\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    @property\n",
    "    def total_p(self):\n",
    "        return self.tree[0]  # the root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prioritized_Replay_DQN(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.step = 0\n",
    "        self.sampling_nums = 0\n",
    "        self.batch_size = 64\n",
    "        self.update_freq = 100\n",
    "        self.experience_nums = 0\n",
    "        self.replay_size = 1000\n",
    "        self.model = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "        self.optimizer = keras.optimizers.Adam(1e-3)\n",
    "\n",
    "        self.tree = SumTree(self.replay_size)\n",
    "        self.epsilon = 0.01  # small amount to avoid zero priority\n",
    "        self.alpha = 0.6  # [0~1] convert the importance of TD error to priority\n",
    "        self.beta = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "        self.beta_increment_per_sampling = 0.001\n",
    "        self.abs_err_upper = 1.  # clipped abs error\n",
    "\n",
    "    def create_model(self):\n",
    "        input = keras.layers.Input(shape=state_dim)\n",
    "        hidden1 = keras.layers.Dense(64, activation='relu')(input)\n",
    "        hidden2 = keras.layers.Dense(16, activation='tanh')(hidden1)\n",
    "        output = keras.layers.Dense(action_dim)(hidden2)\n",
    "        model = keras.models.Model(inputs=[input], outputs=[output])\n",
    "        return model\n",
    "\n",
    "    def choose_action(self, state, epsilon=0.1):\n",
    "        if np.random.uniform() > epsilon - self.step * 0.0001:\n",
    "            return np.argmax(self.model.predict(tf.convert_to_tensor([state], dtype=tf.float32))[0])\n",
    "        else:\n",
    "            return random.sample(list(np.arange(0, action_dim)), 1)[0]\n",
    "\n",
    "    def perceive(self, state, action, state_, reward, done):\n",
    "        # max_p = np.max(self.tree.tree[-self.tree.capacity: ])\n",
    "        # if max_p == 0:\n",
    "        #     max_p = self.abs_err_upper\n",
    "        max_p = np.mean(np.abs(reward + gamma * self.target_model(np.array(state_).reshape(1,state_dim)) \\\n",
    "                       - self.model(np.array(state).reshape(1, state_dim))))\n",
    "        self.tree.add(max_p, np.hstack((state, action, state_, reward, done)))   # set the max p for new p\n",
    "        self.experience_nums += 1\n",
    "        # print(max_p)\n",
    "        # print(self.tree.total_p)\n",
    "\n",
    "    def sample(self, size):\n",
    "        b_idx, b_memory, ISWeights = np.empty((size,), dtype=np.int32), \\\n",
    "                                     np.empty((size, self.tree.data[0].size)), \\\n",
    "                                     np.empty((size, 1))\n",
    "        pri_seg = self.tree.total_p / size       # priority segment\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])  # max = 1\n",
    "\n",
    "        min_prob = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_p     # for later calculate ISweight\n",
    "        if min_prob == 0:\n",
    "            min_prob = 0.00001\n",
    "        for i in range(size):\n",
    "            v = np.random.uniform(pri_seg * i, pri_seg * (i + 1))\n",
    "            idx, p, data = self.tree.get_leaf(v)\n",
    "            prob = p / self.tree.total_p\n",
    "            ISWeights[i, 0] = np.power(prob/min_prob, -self.beta)\n",
    "            b_idx[i], b_memory[i, :] = idx, data\n",
    "        return b_idx, b_memory, ISWeights\n",
    "\n",
    "    def model_train(self):\n",
    "        self.step += 1\n",
    "        if self.step % self.update_freq == 0:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        tree_idx, replay_batch, ISWeights = self.sample(self.batch_size)\n",
    "        state_batch = replay_batch[:, 0:state_dim]\n",
    "        action_batch = replay_batch[:, state_dim:state_dim + 1]\n",
    "        next_state_batch = replay_batch[:, state_dim + 1:state_dim + 1 +state_dim]\n",
    "        reward_batch = replay_batch[:, state_dim + 1 + state_dim:state_dim + 1 + state_dim + 1]\n",
    "        done_batch = replay_batch[:, -1]\n",
    "\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.model.variables)\n",
    "            Q = self.model(tf.convert_to_tensor(state_batch, dtype=tf.float32))\n",
    "            Q_next = self.target_model(tf.convert_to_tensor(next_state_batch, dtype=tf.float32))\n",
    "\n",
    "            max_action = tf.argmax(Q, axis=1)  # Nature_DQN is tf.argmax(Q_next, axis=1)\n",
    "            Q = tf.reduce_sum(tf.one_hot(action_batch, action_dim) * Q, axis=1)\n",
    "            Q_next = tf.reduce_sum(tf.one_hot(max_action, action_dim) * Q_next, axis=1)\n",
    "\n",
    "            target_value = np.array((1 - done_batch) * gamma * Q_next).reshape(64,1) + reward_batch\n",
    "            loss = tf.reduce_mean(ISWeights * tf.square(Q - target_value) * 0.5)\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    agent = Prioritized_Replay_DQN()\n",
    "    score_list = []\n",
    "    for episode in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        while True:\n",
    "            action = agent.choose_action(state)\n",
    "            if render:\n",
    "                env.render()\n",
    "            state_, reward, done, _ = env.step(action)\n",
    "            agent.perceive(state, action, state_, reward, done)\n",
    "            if agent.experience_nums >= agent.replay_size:\n",
    "                agent.model_train()\n",
    "            state = state_\n",
    "            score += reward\n",
    "            if done:\n",
    "                score_list.append(score)\n",
    "                print('episode:', episode, 'score:', score, 'max_score:', np.max(score_list))\n",
    "                if agent.experience_nums >= agent.replay_size:\n",
    "                    print(\"   Training   ....\")\n",
    "                break\n",
    "        if np.mean(score_list[-10:]) > 180:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    plt.plot(score_list, color='orange')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
