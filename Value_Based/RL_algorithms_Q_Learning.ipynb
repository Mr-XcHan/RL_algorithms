{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    algorithm：Q-Learing\n",
    "               Q(s,a) <- Q(s,a) + alpha(r + gamma * maxa_Q(s_, a_) - Q(s, a))\n",
    "               \n",
    "    environment：FrozenLake-v0\n",
    "    \n",
    "    author: Xinchen Han\n",
    "    date: 2020/7/25\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg_name = 'Q_Learning'\n",
    "env_id = 'FrozenLake-v0'\n",
    "env = gym.make(env_id)\n",
    "\n",
    "## Set hyperparameters\n",
    "epsilon = .8\n",
    "alpha = .8  \n",
    "gamma = .9  # decay factor\n",
    "max_episodes = 20000\n",
    "t0 = time.time()\n",
    "\n",
    "Q_table = np.zeros([env.observation_space.n, env.action_space.n], dtype = np.float64)\n",
    "reward_buffer = [0] # In case the error: list index out of range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    \"\"\"\n",
    "        take action policy：epsilon-greedy\n",
    "    \"\"\"\n",
    "    if (np.random.rand() > epsilon) or ((Q_table[state, :] == 0)).all():\n",
    "        action = np.random.choice(env.action_space.n)\n",
    "    else:\n",
    "        action = np.argmax(Q_table[state,:])\n",
    "    return action\n",
    "\n",
    "def Q_Learning():\n",
    "    for episode in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        epi_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "#             action = choose_action(state)  # the epsilon-greedy policy is worse than the nosiy_greedy policy\n",
    "            action = np.argmax(Q_table[state, :] + np.random.randn(1, env.action_space.n) * (1. / (episode + 1)))\n",
    "            state_, reward, done, _ = env.step(action)\n",
    "            Q_table[state][action] = Q_table[state][action] + \\\n",
    "                                        alpha * (reward + gamma * np.max(Q_table[state_, :]) - Q_table[state][action])\n",
    "            state = state_\n",
    "            epi_reward += reward\n",
    "        reward_buffer.append(reward_buffer[-1] * 0.9 + epi_reward * 0.1)\n",
    "        print(\n",
    "                'Training  | Episode: {}/{}  | Reward:{: .4f} |Running Time: {:.4f}'.format(\n",
    "                episode + 1, max_episodes, epi_reward, time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_Learning()\n",
    "plt.plot(reward_buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
